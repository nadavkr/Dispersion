\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}


\usepackage[dvipsnames,usenames]{color}
\usepackage[colorlinks=true,urlcolor=Blue,citecolor=Green,linkcolor=BrickRed]{hyperref}
\usepackage[usenames,dvipsnames]{xcolor}
\urlstyle{same}


\usepackage{amsthm,amsmath,amssymb, mathabx}
\usepackage{fullpage}
\usepackage[ruled,noend,linesnumbered]{algorithm2e}     
\usepackage{enumerate,comment}
\usepackage{url}
\usepackage[capitalise]{cleveref}
\usepackage{todonotes}
\usepackage{tikz}
\usepackage[noadjust]{cite}

\usepackage{graphicx, subfigure}
\usepackage{array}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsfonts}

\usepackage{color}

%\usepackage{algorithm}      % http://ctan.org/pkg/algorithms
%\usepackage{algpseudocode}  % http://ctan.org/pkg/algorithmicx
\usetikzlibrary{trees}

%\pagestyle{plain}

\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\poly}{\text{poly}}

\newcommand{\Oh}{{O}}
\newcommand{\common}{\mathsf{common}}
\newcommand{\encode}{\mathcal{L}}
\newcommand{\position}{\mathsf{position}}
\newcommand{\leaves}{\mathsf{leaves}}
\newcommand{\diff}{\mathsf{diff}}
\newcommand{\id}{\mathsf{id}}
\newcommand{\lightdepth}{\mathsf{lightdepth}}
\newcommand{{\appx}}[2][2]{\lfloor{#2}\rfloor _{#1}}
\newcommand{\pre}{\mathsf{pre}}
\newcommand{\lightrange}[1]{\mathsf{L}_{#1}}
\newcommand{\heavy}{\mathsf{heavy}}
\newcommand{\lightsize}{\mathsf{ligthsize}}
\newcommand{\apex}{\mathsf{apex}}
\newcommand{\size}{\mathsf{size}}
\newcommand{\NCA}{\mathsf{NCA}}
\newcommand{\nca}{\text{NCA}}
\newcommand{\NCSA}{\mathsf{NCSA}}
\newcommand{\NCH}{\mathsf{NCH}}
\newcommand{\pow}{\mathsf{pow}}
\newcommand{\distance}{\mathsf{d}}
\newcommand{\nil}{\mathsf{nil}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\children}{\mathsf{children}}
\newcommand{\parent}{\mathsf{parent}}

\newcommand{\dist}{\text{dist}}
\newcommand{\tlab}{L}
\newcommand{\tlabmax}{M}
\newcommand{\tlabbound}{\mathcal{M}}
\newcommand{\scheme}{A}
\newcommand{\allschemes}{\mathcal{A}}
\newcommand{\lightcount}{\lightdepth}
\newcommand{\ncadist}{\mathsf{NCA}\text{-}\mathsf{distance}}
\newcommand{\treeroot}{\mathsf{root}}
\newcommand{\collapsed}{\mathcal{C}}
\newcommand{\hphead}{\mathsf{head}}



% For cleverref compatibility
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algo}{Algorithm}[section]
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{observation}{Observation}
\newtheorem{fact}[theorem]{Fact}
\theoremstyle{definition}   
\newtheorem{definition}{Definition}
\usepackage{authblk}
\theoremstyle{remark}
\newtheorem{example}[theorem]{Example}
\newtheorem*{claim}{Claim}
\newtheorem{case}{Case}
\newtheorem{step}{Step}[section]

\newtheorem{property}[theorem]{Property}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\begin{document}

\title{Dispersion on Trees}
\author[1]{}
\affil[1]{University of Haifa, Israel}


\date{}
\maketitle

\begin{abstract}
bla bla 
\end{abstract}

\section{Introduction}
Facility location is a family of problems, where the goal is to place
a number of facilities as to minimize the total cost while preserving
given constraints. Its basic version, called the metric k-center
problem, we want to designate up to k nodes of a given weighted graph
to be facilities, as to minimize the maximum distance of a node to its
closes facility. Even this simplest version is already NP-hard \cite{Vazirani2003}. However, a simple greedy 2-approximation algorithm is known \cite{Gonzalez1985}. The situation for different
objective functions or more restrictions on the placement is of course
more complex, but nevertheless one can often design an efficient
approximation algorithm, see \cite{DavidB.Shmoys1997} and the references
therein.

Given that facility location problems are usually if not always
NP-hard, it makes sense to consider them on a restricted family of
graphs. The simplest yet still non-trivial such a family are trees on
$n$ nodes. There are multiple possible versions of the k-center
problem on trees: the facilities can be either only the nodes of the
tree or any points on an edge, and we might either minimize the
distance of any node or any point on an edge to its closest facility.
All these versions have been extensively studied, culminating in an
O(n) time and space algorithm given by Frederickson \cite{Frederickson1991a} that
solves all but one of them. This was an improvement on the previous
$O(n\log n)$ time algorithm by Frederickson and Johnson \cite{Frederickson1983}, which in turn was an improvement
on the $O(n\log^2n)$ time algorithm \cite{Megiddo1981}. There is also weighted version
of the problem, where every node has its associated weight and the
distance is multiplied by the corresponding weight, has been
considered, and $O(n\log^2n)$ time algorithm is known %cite Cole and
\cite{Megiddo1983}. Among other facility location problems on trees there
is the max-min tree k-partitioning \cite{Perl1981} and the min-max tree
k-partitioning \cite{Becker1982}, both solved by Frederickson in $O(n)$ time
\cite{Frederickson1991} using a clever approach based on the parametric search that
he has later extended to solve the k-center problem.

We consider another problem that falls within the class of facility
location problem on trees. In the max-min tree k-dispersion the goal
is to select k nodes as to maximize the minimum distance between any
two chosen nodes. This nicely generalizes the maximum independent set
problem by binary searching for the largest value of k for which the
minimum distance is at least 2. The best previously known algorithm
for the max-min tree k-dispersion was given by Bhattacharya and Houle in
\cite{Bhattacharya1991} and takes $O(n\log n)$ time, after Wang and Kuo solved the problem in the one dimensional case with a running time of $O(np + nlogn)$ in \cite{Wang1988}. Using the
framework of Frederickson, we construct an optimal O(n) time algorithm
for this problem. In our solution we also develop a slightly modified
version of this framework, which might be simpler to understand. Then
we move the weighted version, where the goal is to select nodes with
the total weight being at last k. There, the best previously known
solution requires $O(n\log^4 n)$ time \cite{Bhattacharya1991}. We present a significantly
faster $O(n\log^2 n)$ time algorithm. We also show that the decision
version, where the goal is to check if there exists a set of of nodes
total weight k such that any two nodes are at distance at least b,
requires $\Theta(n\log n)$ time in the algebraic decision tree model. We
also present a matching $O(n\log n)$ time algorithm, which is then used
to construct our $O(n\log^2n)$ time solution for the weighted max-min
tree k-dispersion using the standard parametric search paradigm. 

\section{Preliminaries}
\begin{definition}
\emph{Max-Min Dispersion Problem (MMDP)} Given a tree $T$ with non-negative edge lengths, and a natural number $p$, find a subset $P\subseteq V$ of size $p$ s.t. $f(P)=min{\scriptscriptstyle \forall u,v\in P,u\neq v}d(u,v)$ is maximized.
\end{definition}
\begin{definition}
\emph{MMDP Feasibility Test} Given a MMDP instance and a number $\lambda\geq0$ decide if there exists a subset $P\subseteq V$ of size $p$ s.t. $f(P)\geq\lambda$.
\end{definition}

\section{The unweighted case}
We first describe how to solve the feasibility test in linear time, and then use this algorithm to solve the MMDP.
\subsection{Linear algorithm for the MMDP Feasibility Test}
%\begin{algo} \label{unWeightedFeasibilityAlgo}
We show a recursive linear algorithm.
At each step of the recursion, we would like to provide the maximal valid solution for the current subtree, given some $\lambda$. I.e, we would like to return $P$, a maximal subset of the vertices of the subtree, s.t. $f(P)\geq\lambda$.
We are given a root vertex $v$ and its children nodes $v_{1},v_{2},...,v_{k}$, and for each child we are given a maximal valid solution for the MMDP feasibility test on its subtree. We would like to produce a maximal valid solution for the feasibility test on $v\text{'s subtree}$.
Denote by $P_{1},...,P_{k}$ the solutions for the feasibility test on each subtree rooted at a child of $v$.
For any subtree of $T$ rooted at node $r$, and a valid solution $P$ for the MMDP feasibility test on the subtree, we call a node $u\in P$, s.t. $d(r,u)\leq\frac{\lambda}{2}$, a \textcolor{blue}{blue} node of the subtree. We call the vertex in $P$ that is closest to $r$, but isn't blue, \textcolor{green}{green}.
Note that each $P_{i}$ contains at most one blue vertex and one green vertex.

\paragraph{The recursion step} Given $P_{1},...,P_{k}$, we would like to produce a solution for $v$'s subtree.
\begin{enumerate}
\item Put in $P$ all the vertices in $P_{1},...,P_{k}$, except for the blue vertices.
\item Take all blue nodes $u$ s.t. $d(u,v)> \frac{\lambda}{2}$
\item Take $u'$, the blue node farthest from $v$ s.t. $d(u',v)\leq \frac{\lambda}{2}$, if it exists, and if $d(u',x)\geq \lambda$, where $x$ is the closest node to the root $v$ we have chosen so far.
\item Decide whether to take $v$ (the root) to the solution by looking at the closest vertex to it we have already put in $P$.
\end{enumerate}


\begin{definition}
\emph{Active and Inactive nodes} We will call a vertex $v$ of the tree \emph{inactive} if there is a vertex $u$ we already chose to be in $P$, s.t $d(u,v)<\lambda$. We will call $v$ \emph{active} if there is no such vertex $u$ and $v \notin P$.
\end{definition}
\begin{lemma} \label{greenNodesLemma}
For any subtree of $T$ rooted at node $r$, let $u$ be the most distant active vertex from $r$ in the subtree. Assume $\exists P \subseteq V$ s.t. $f(P)\geq\lambda$, and denote the closest vertex to $u$ in $P$ by $u'$. Then, if $d(u,r)\ge\frac{\lambda}{2}$, and $P' := P \setminus \lbrace u' \rbrace \cup \lbrace u \rbrace$, it holds that $f(P')\geq\lambda$.
\end{lemma}
\begin{proof}
Assume for contradiction that $f(P')<\lambda$. We assume that we cannot replace $u'$ with $u$ and still get a valid solution. This means that there is a vertex $x \neq u'$, s.t. $x\in P$ and $d(u,x)<\lambda$. 
We know that:
\observation{ $d(r,u)\ge\frac{\lambda}{2}$ (by definition)}
\observation{ $d(u,u')<\lambda$ (because otherwise we could definitely add $u$ to the solution, as $u'$ is defined as closest to $u$ in $P$)}
\observation{ $d(x,u')\ge\lambda$ (since $x$ and $u'$ are both in $P$)}
\observation{ $d(x,u)<\lambda$ (by definition)}
%\corollary{\label{corol5} $d(r,u)>d(r,u')$ (since otherwise $u$ is not the active vertex most distant from $r$).}
\\ {\normalfont Let us look at the possible cases:
\begin{enumerate}
\item \textit{Both $u'$ and $x$ are in the subtree rooted at $r$:}
Because $u$ is the lowest active node $r$'s subtree, it cannot be an ancestor of $u'$ or $x$.
Let $z=lca(u,u')$. We know that $d(u,z)\geq d(u',z)$ (because $u$ is farther from $r$ than $u'$). Observe two possible cases:
\begin{enumerate}
\item \emph{The path from $u$ to $x$ passes through $z$:}
$\lambda> d(u,x) = d(u,z) + d(z,x) \geq d(u',z)+d(z,x) = d(u',x)$ in contradiction to observation 3.
\item \emph{The path from $u$ to $x$ does not pass through $z$:}
In this case $z':=lca(u,x)$ must be a descendant of $z$. It holds that $d(x,z')\leq d(u,z')$.
Also, $\lambda \leq d(u',x) \leq d(u',z')+d(x,z') \leq d(u',z')+d(u,z') = d(u',u)$ in contradiction to observation 2.
\end{enumerate}

\item \textit{$u'$ is in the subtree, and $x$ is not:}
This means that the paths from $u$ and $u'$ to $x$ go through $r$. We have that $d(r,u)>d(r,u')$ (since otherwise $u$ is not the active vertex most distant from $r$), which implies that $d(r,u)+d(r,x)>d(r,u')+d(r,x)\Rightarrow d(u,x)>d(u',x)$ in contradiction to observations 3 and 4.

\item \textit{$x$ is in the subtree and $u'$ is not:}
In this case we know that the paths from $u'$ to $x$ and $u$ go through $r$.
This means that $d(u,u') = d(u,r)+d(r,u')$ and $d(x,u') = d(x,r)+d(r,u')$ $\Rightarrow d(x,r)>d(u,r)$ (because by observation 2 and 3 $d(x,u')>d(u,u')$), but this is a contradiction to $u$ being the lowest active node in the subtree rooted at $r$.

\item \textit{Both $x$ and $u'$ are not in the subtree:}
In this case the paths from $u$ to $u'$ and $x$ got through $r$. $d(u,u') = d(r,u)+d(r,u') < \lambda$ (due to observation 2) $\Rightarrow d(r,u')<\frac{\lambda}{2}$ (due to observation 1). In addition we have that $d(u,x) = d(r,u)+d(r,x) < \lambda$ (due to observation 4) $\Rightarrow d(r,x)<\frac{\lambda}{2}$. It is clear that $d(u',x) \leq d(u',r)+d(r,x) < \lambda$ in contradiction to observation 3.
\end{enumerate}
}
\end{proof}
%TODO add a drawing for each case
Lemma \ref{greenNodesLemma} implies the correctness of the algorithm.
%TODO extend proof of correctness


\subsection{Sublinear feasibility test for the MMDP}
The main idea here is partitioning the tree to fragments and preprocessing them, so that in query time we can process fragments in sub-linear time. We will assume the tree is binary. \footnote{Notice that throughout our algorithm we process caterpillars, and do not consider taking spine nodes to our solution. This is okay, since we can add some artificial nodes with zero-weighted edges. This is also how we handle non-binary trees.}
\subsubsection{Tree Partitioning}
We would like to have fragments of size $b=log^2n$, s.t. each fragment will be connected to the rest of the tree by at most two vertices. Each fragment has at most $2b$ vertices, and will be defined by its border vertices $u$ and $v$, s.t. $v$ is a descendant of $u$, as in the figure. The fragment will consist of $u$'s subtree without $v$'s subtree. We call the path from $u$ to $v$, the fragment's \textit{spine}, and $v$'s subtree the \textit{hole} of the fragment. %TODO add figure
\begin{lemma}
For any binary tree on $n$ nodes and a parameter $b$, there is a partition of the tree into $O(n/b)$ such fragments of size at most $2b$.
\end{lemma}
\begin{proof}
Call a node $u$ \textit{large} if the size of its subtree is greater than b and \textit{small}
otherwise. Consider the tree $T_L$ induced by the large nodes of the original tree. Make each leaf in $T_L$ a new fragment with no holes. This creates $O(n/b)$ fragments since each leaf of $T_L$ is the root of a subtree of size at least $b$ in the original tree, and so we have at most $O(n/b)$ leaves in $T_L$. The size of each of these fragments is at most $2b$, since we assume that the tree is binary.
Next, for every branching node in $T_L$, make it a fragment consisting of just one node. This also creates $O(n/b)$ fragments, since in any tree the number of branching nodes is at most the number of leaves. We could not have simply gone up the tree and cut fragments of size $b$, since this would create fragments with more than two boundary nodes.
Ignoring the fragments we have already created, we are left with nodes that form unary chains in $T_L$. Each of these nodes might also have a child that is a small node. We have $O(n/b)$ of these subgraphs (because each of them is defined by two of the previously defined fragments). We scan each of these subgraphs' large nodes bottom-up, and greedily cut them. Clearly these fragments are of size at most $2b$, as this is where we choose to cut. Denote the size of subgraph $i$, as $b_i$. The number of fragments we created in this phase is $$\sum_{i=1}^{n/b} \left\lceil \frac{b_i}{b} \right\rceil \leq \frac{n}{b}$$
In total we have created $O(n/b)$ fragments, each of which is of size at most $2b$.
\end{proof}
This partitioning is done in $O(n)$ time.
We now would like to pre-process the fragments s.t. we can later check in constant time for any two nodes in a fragment, $u_1,u_2$ if $d(u,v)\geq\lambda$, and if $d(u_1,u_2) \leq \frac{\lambda}{2}$, for any possible $\lambda$. We achieve this for most of the fragments.

\subsubsection{Pre-Processing Fragments}
For each fragment, we would like to implicitly construct matrices of total side length (number of rows and columns) $O(blogb)$, that are row and column sorted, and contain all pairwise distances in the fragment.

\begin{definition} \textbf{Centroid Decomposition} %TODO citation?
Given a binary tree, we can find its \textit{centroid} node in linear time. This node divides the tree into three pieces, each of which has no more than half of the nodes of the entire tree. Recursing on each of the pieces will produce the centroid decomposition.
\end{definition}

Apply the centroid decomposition on each fragment. Now, run the following recursive routine: at each level of the recursion, we are given 3 pieces, and for each of them a sorted list of the distances to the inner centroid. We would like to compute a sorted list of distances of all nodes of the current piece to the current centroid. Let us look at one of the three pieces we have already processed. It contains three pieces of its own. For two of these pieces, it holds that all paths from a node inside them to the outer centroid passes through the inner centroid. We call the third piece, for which this does not hold a \textit{problematic} piece. For the two pieces that are not problematic, we can add to all the entries of the lists we have, the distance from the inner centroid to the outer centroid, and then merge the two sorted lists. This is all done in linear time.\\
Our only problem now is the piece where the paths to the outer centroid do not go through the inner one. But notice that if you go one step deeper in the recursion you can see the inside the problematic piece there are two pieces that are not problematic, and we can do the same for their two list and so on. This recursive routine will also take linear time. All together we have $O(logb)$ levels of recursion, and spend linear time on each one, so we get a running time of $O(blogb)=O(nloglogn)$.\\
Now we have for level $i$ of the recursion $3^i$ centroids, and for each of them a sorted list of the distances to it from all nodes in its piece. We could find any pairwise distance in a piece by adding two entries of this list. Since the lists are sorted, we can look at the situation as if we have $3^i$ matrices of and their total side length is $n$. Looking at the matrices from all $O(logb)$ levels of the recursion, the total side length is $O(blogb)$. These matrices contain all pairwise distances in all the fragments. Note that we also have some entries in the matrices that do not actually represent pairwise distances, but that does not hurt us.
Since we would like to also answer queries for $\frac{\lambda}{2}$, we do the same process for matrices that contain twice the distance in every entry of the original matrices.\\

Now, we can use Frederickson's serching algorithm to eliminate entries of the matrices. Using this searching method, we find an interval $[\lambda_1,\lambda_2)$, s.t. the $\lambda^*$ we are searching for in the MMDP, is in this interval.\\
We call fragments that do not contain a pairwise distance that is in the interval \textit{inactive}, and all other fragments \textit{active}.
For this search, we use theorem 2.1 in \cite{Frederickson1991}:
\begin{theorem}
Let $M$ be a collection of $N$ sorted matrices ${M_l, M_2, . . . , M_N}$ in which matrix $M_j$ is of dimension $m_j \times nj$, $mj \leq nj$, and $\sum_{j=1}^{N} mj = m$.
Let p be nonnegative. The number of feasibility tests needed to discard all but at most p of the elements is $O(max \lbrace log (max_j \lbrace nj \rbrace), log(m/(p+ 1)) \rbrace )$, and the total running time exclusive of feasibility tests is $O(\sum_{j=1}^{N} m_j \times 1og(2n_j/m_j))$.
\end{theorem}
In our case $m=blogb$ (since we have $logb$ levels of the centroid decomposition) and we set $p$ to be $n/b^2$. The theorem implies that we can use $O(logb)$ feasibility tests and discard all but $n/b^2$ elements of the matrices. We also pay $O(nlogb)$ exclusive of the feasibility tests. This means that we have at most $n/b^2$ active fragments (out of all $n/b$ fragments). \\
For inactive fragment, it is clear that we can answer queries checking whether some pairwise distance inside the fragment is at least $\lambda$, or if it is at most $\frac{\lambda}{2}$.

Active fragments, will be processed as in the linear algorithm described before, so we can ignore them for now.

We now do the following pre-processing for each inactive fragment:
\begin{step}
\textbf{reduce the fragment to a caterpillar:}
Observe that each fragment is comprised of its spine, and the subtrees hanging off of it. Using queries on pairwise distances as described above, we can use our linear feasibility test on the subtrees hanging off the spine, and get a blue and a green node for each of them. This is done in linear time.\\
Our fragment is now reduced to caterpillar with a blue child and a green child for every spine node.
\end{step}
\begin{step}\label{removing green nodes}
\textbf{Find the blue nodes that will certainly not be taken:}
Let the $i$-th leaf be connected by an edge of length $y_i$ to a spine node at distance $x_i$ from the root of the caterpillar. Order the leaves so that $x_1 < x_2 < ... < x_k$.
Some of the blue nodes can be ignored as they "collide" with green nodes. We start by finding the closest green node to each blue node. We can do this in linear time by scanning the caterpillar bottom-up, while saving for each blue node, the closest green node below it and the distance to it. We then do the same scan from top to bottom, and from both scans combined, we get for each blue node the closest green node and the distance to it. Now we want to find the blue nodes that are too close to a green node. Find the median of the distances, run a feasibility test on it, and repeat for the remaining distances, while updating the interval. Delete all blue nodes for which the distance to the closest green node is smaller than the new $\lambda_1$. Notice that after this median routine, the distance between any blue node and the closest green node to it does not fall inside the interval. Thus, we can know for certain whether a specific blue node is too close to any green node, or not.
Now we are only left with some of the blue nodes to consider, and for all of them it hold that $y_i < \lambda_1/2$, and from this point we can ignore the green nodes.
\end{step}
\begin{step}
\textbf{Prune the caterpillar so that the leaves' distances to the root are non-decreasing:}
Traverse the caterpillar from bottom to top. Observe the $i$-th leaf, $u$. Denote by $v$ the leaf above $u$, and let  $j=i-1$. Assume $x_j+y_j > x_i+y_i$ (i.e $v$ is farther from the root than $u$). We get that $x_i-x_j+y_i < y_j$. $d(u,v) = x_i-x_j+y_i+y_j < 2y_j < \lambda$. So an optimal solution cannot contain both $u$ and $v$. If $u$ is in the solution we can replace it with $v$. This can be done because $v$ is farther away from any node above it than $u$, and $u$ is closer to any node below it than $v$. Let $v'$ be a leaf node above $v$, then $d(v,v') - d(v',u) \geq y_j-(x_i-x_j)-y_i = x_j+y_j-(x_i+y_i) > 0$, and the reasoning for the second case is similar.
So in fact, we can remove the i-th leaf from the caterpillar.
\end{step}
\begin{step}
\textbf{Prune the caterpillar so that the leaves' distances to the bottom are non-decreasing:}
This is done as in the previous step.
\end{step}
\begin{step}
\textbf{Compute the solution for any possible value of the nearest chosen node in the hole:}
In query time we will want to process each fragment after its hole has already been processed.\\
We call a set of consecutive leaves of the caterpillar, that contains the closest leaf to the hole, a \emph{suffix} of the caterpillar. Similarly we can define the caterpillar's \emph{prefix}.\\
Assuming we know what is the closest chosen node in the fragment's hole, we can eliminate an entire suffix of the caterpillar (because of the monotonicity of the distances of blue nodes from the hole). We can compute in preprocessing, for any possible eliminated suffix, which nodes in the fragment should be chosen, and also which nodes are the blue and green node we need to pass to the next fragment.\\
In order to pre-compute this, scan the caterpillar from top to bottom. For every possible prefix of the caterpillar we would like to store the numbers of nodes chosen, the green node and the blue node, assuming all nodes that are not in the prefix are eliminated as they are too close to the hole. Observe the bottom node of the current prefix. If the distance of this node to the fragment's root is less than $\frac{\lambda}{2}$\footnote{We do not actually know $\lambda$ in preprocessing, but due to the previous steps, such a query of a node inside the fragment would produce the same result for any possible $\lambda$.}, store it as the blue node, zero as the number of chosen nodes, and NULL as the green node (if this fragment has a hole you also need to consider its blue and green node). Else, (the node's distance to the root is more than $\frac{\lambda}{2}$), write it as the green node. Then, use binary search to find the node closest to it, that is at distance of at least $\lambda$. Add 1 to the number of chosen nodes stored in the node you found, and either store it as the blue node, or copy the blue node stored there. This would take $O(blogb)$ time. We can improve this by not binary searching for the first node far enough, but instead saving it for each prefix, and then only go down from the previous node found. This would only take $O(b)$ time.
\end{step}

\subsubsection{Feasibility Test}
We now scan the whole tree, and for each inactive fragment we would like to produce in $O(logb)$ time: the number of chosen nodes in the fragment, the blue node, and the green node. In this procedure we use the caterpillars we have created in the preprocessing. Notice that now we are given $\lambda$.
We run this procedure bottom-up, so we assume the fragment rooted at the hole of the current one has already been processed.\\
Denote the root of the fragment, $r$, the hole's blue node, $u$, and the current fragment's leaf farthest from $r$, $v$.\\
\begin{case}
\textbf{$d(r,u)\leq \frac{\lambda}{2}$:} Because of the monotonicity of the distances of leaves from the hole, every leaf of the caterpillar is closer to $r$ than $u$, so we return $u$ as the blue node, and the same green node as we have for the hole, and number of chosen nodes as we had before.
\end{case}
\begin{case}
\textbf{$d(r,u) > \frac{\lambda}{2}$:}
This means we can ignore the hole's green node (as it will surely be taken), and take $u$ as the green node. Now we need to compute the suffix of the fragment that cannot be taken. We can do this by binary searching for the first node of the fragment that is at distance at least $\lambda$ from $u$. We have pre-computed all we need to know once we have the relevant prefix.
\end{case}
Each feasibility test will cost us $O(logb)$ for each inactive fragment, and $O(b)$ for each active one. In total $O(\frac{n}{b} \times logb) + O(\frac{n}{b}) = O(\frac{nloglogn}{log^2n})$.


\subsection{Solution for the MMDP}
The general idea of the algorithm is using the heavy path decomposition. We are searching for $\lambda^*$, which is the distance between some two nodes in the tree, and the largest number for which the MMDP feasibility test would return true.
We go through the heavy path tree bottom-up, and process all heavy paths at a specific level in parallel, until we are left with a green and blue node for each of them. We maintain an interval $[\lambda_1,\lambda_2)$ s.t. $\lambda^*$ is in the interval, that gets smaller and smaller throughout the run.

We now describe the processing of a heavy path. Notice that the traversal is bottom-up, so we have already processed all of its children in the heavy path tree. Because we have determined $\lambda^*$ with sufficient accuracy (i.e. reduced the size of the maintained interval), each subtree hanging off a heavy path can be replaced by its blue and green node attached by single edges to the heavy path. Hence, each such heavy path is a caterpillar, with one or two children for each spine node. We would like to be able to find the green and blue nodes for this heavy path. %The situation is illustrated in figure %TODO add a figure
Assume that the caterpillar has k nodes. Let the $i$-th leaf be connected by an edge of length $y_i$ to a spine node at distance $x_i$ from the root of the caterpillar. Order the leaves so that $x_1 < x_2 < ... < x_k$.

\begin{step}
\textbf{Find the blue nodes that will certainly not be taken:}
Some of the blue nodes can be ignored as they "collide" with green nodes. We start by finding the closest green node to each blue node. We can do this in linear time by scanning the caterpillar bottom-up, while saving for each blue, the closest green node below it and the distance to it. We the do the same scan from top to bottom, and get for each blue node the closest green node and the distance to it. Now we want to find the blue nodes that are too close to a green node. Find the median of the distances, run a feasibility test on it, and repeat for the remaining distances, while updating the interval. Delete all blue nodes for which the distance to the closest green node is smaller than $\lambda^*$.
Notice that we are only left with some of the blue nodes to consider, and for all of them it hold that $y_i < \lambda_1/2$, and from this point we can ignore the green nodes.
\end{step}
\begin{step}
\textbf{Prune the caterpillar so that $\pmb{x_i+y_i}$  is non-decreasing:}
Traverse the caterpillar from bottom to top. Observe the $i$-th leaf, $u$. Denote by $v$ the leaf above $u$, and let  $j=i-1$. Assume $x_j+y_j > x_i+y_i$ (i.e $v$ is farther from the root than $u$). We get that $x_i-x_j+y_i < y_j$. $d(u,v) = x_i-x_j+y_i+y_j < 2y_j < \lambda$. So an optimal solution cannot contain both $u$ and $v$. If $u$ is in the solution we can replace it with $v$. This is because $v$ is farther away from any node above it than $u$, and $u$ is closer to any node below it than $v$. Let $v'$ be a leaf node above $v$, then $d(v,v') - d(v',u) = y_j-(x_i-x_j)-y_i = x_j+y_j-(x_i+y_i) > 0$, and the reasoning for the second case is similar.
So in fact, we can remove the i-th leaf from the caterpillar.
\end{step}
\begin{step}
\textbf{Construct a row and column sorted matrix storing all pairwise distance in the caterpillar}
It is easy to see that arranging the matrix in the natural order will produce a triangular matrix with monotone rows and columns.
\end{step}
\begin{step}
\textbf{Run a searching algorithm on the sorted matrices of all the heavy paths}
We now use Frederickson's method to search our sorted matrix and eliminate its entries using $O(logk)$ calls to out feasibility test algorithm. We thus narrow down our interval to be $[\lambda_1,\lambda_2)$ and $\nexists x\in [\lambda_1,\lambda_2)$ s.t. x is a pairwise distance in the caterpillar, and so we can mark a blue and a green node of the caterpillar that will be valid for any value of $\lambda^*$.  %TODO citation needed.
\end{step}
After processing all heavy paths we finally a maximal valid solution for the whole tree. Using the sublinear feasibility test we introduced, this is done in O(n) time.


\subsection{$\textbf{O(nlog*n)}$ Algorithm}
The bottleneck of the algorithm that we have presented is the $O(nloglogn)$ pre-processing we use to make sub-linear feasibility test possible. We did this by dividing the input tree to fragments of size $b$, and using the linear feasibility test to eliminate possible values of $\lambda^*$. This elimination process needed to use $O(logb)$ calls to the feasibility test procedure.
\\We will now use a simple bootstrapping technique to lower the preprocessing time by iteratively using growing fragment sizes, each time improving the complexity of our feasibility test (f.t.).\\
We start with fragments of size $c^2$ for some constant $c$.
We pay $O(nlog(c^2)) = O(n)$ time for preprocessing and get an $O(\frac{n}{c^2} \times logc)$ time f.t.
we use this f.t. to do the same preprocessing again, this time with fragments of size $(2^c)^2$.
We will now pay $O(log(2^{2c}) \times \frac{nlogc}{c^2})= O(\frac{nlogc}{c})$ for preprocessing and get an $O(\frac{n}{2^{2c}} \times c)$ f.t.
We do this for $log*n$ iterations until we get to fragments of size $log^2n$.
Generally, we want to take a tester for fragments size of $(log^{i+1}n)^2$ (iterated logarithm), do linear time processing, and get a f.t. for fragments size of $(log^in)^2$. We have a tester that works in $O(\frac{nlog^{i+2}n}/{log^{i+1}n)^2}$, and now we use it for fragments size of $(log^in)^2$. We do $log((log^in)^2) = log^{i+1}n$   steps of matrix elimination using our f.t., which costs in total $O(log^{i+1}n \times ((nlog^{i+2}n)/(log^{i+1}n)^2)) = O(nlog^{i+2}n/log^{i+1}n)$, (less than linear time), and we get a f.t. that works in $O(\frac{nlog^{i+1}n}/{log^in)^2}$.
We do this for $log^*n$ iterations until we get to fragments of size $log^2n$, and have a f.t. of $O(\frac{nloglogn}{log^2n})$. Since we did $log^*n$ iterations and at most linear time work at each of them the total preprocessing is in $O(nlog^*n)$ time.



\bibliographystyle{alpha}
\bibliography{dispersion}



\end{document}